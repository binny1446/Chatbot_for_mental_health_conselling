{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16tj14xVrHCP"
      },
      "source": [
        "# Overview\n",
        "\n",
        " We have several notebooks to introduce Transformer like:\n",
        "\n",
        " * [Encoder in Transformer](https://www.kaggle.com/code/aisuko/encoder-in-transformers-architecture)\n",
        " * [Decoder in Transformer](https://www.kaggle.com/code/aisuko/decoder-in-transformers-architecture)\n",
        " * [Multiple Head Attention](https://www.kaggle.com/code/aisuko/mask-multi-multi-head-attention)\n",
        "\n",
        "![](https://cdn.masto.host/sigmoidsocial/media_attachments/files/111/973/089/747/694/067/original/3de6c0032f0a7e53.webp)\n",
        "\n",
        "The architecture was first coined in 2017 by Google researches in the \"Attention is All you need\" paper. It is quite revolutionary as previous models used to do sequence-to-sequences learning(machine translation, speech-to-text, etc...) relied on RNNs which were computationnally expensive in the sense they had to process sequences step by step, whereas Transformers only need to look once at the whole sequence, moving the time complexity from $O(n)$ to $O(1)$.\n",
        "\n",
        "![](https://cdn.masto.host/sigmoidsocial/media_attachments/files/112/012/999/887/099/116/original/aa9be4499ab4b301.webp)\n",
        "\n",
        "\n",
        "## Encoder\n",
        "\n",
        "It has a `Multi-Head Attention` mechanism and a fully connected `Feed-Forward network`. There are also residual connections around two sub-layers, plus layer normalization for the output of each sub-layer. All sub-layers in the model and the embedding layers produce outputs of dimension $d_{model}=512$.\n",
        "\n",
        "\n",
        "## Decoder\n",
        "\n",
        "The decoder follows a similar structure, but it inserts a third sub-layer taht performs multi-head attention over the output of the encoder block. There is also a modification of the self-attention sub-layer in the decoder block to avoid positions from attending to subsequent positions. This masking ensures that the predictions for position `i` depend solely on the known outputs at positions less than i.\n",
        "\n",
        "Both the encoder and decoder blocks are repeated N times. In the original paper, it is N=6, and we will define a similar value in this notebook.\n",
        "\n",
        "\n",
        "## Attention Mechanism\n",
        "\n",
        "Attention is a mechanism which is actually not specific to transformer, and which was already used in RNN seq-to-seq models.\n",
        "\n",
        "![](https://cdn.masto.host/sigmoidsocial/media_attachments/files/112/013/014/511/337/184/original/b577a9e08c501872.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Ihm6AfHrHCX"
      },
      "source": [
        "# Input Embeddings\n",
        "\n",
        "The `InputEmbeddings` class below is responsible for converting the input text into numerical vectors of `d_model` dimensions. To prevent that our input embeddings become extremely small, we normalize them by multiplying them by the $\\sqrt{d_{model}}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tKh_WJ6srHCZ"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import torch\n",
        "from typing import Any\n",
        "import torch.nn as nn\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    torch.backends.cudnn.deterministic=True\n",
        "\n",
        "class InputEmbeddings(nn.Module):\n",
        "    def __init__(self, d_model: int, vocab_size: int):\n",
        "        super().__init__()\n",
        "        self.d_model=d_model # Dimension of vectors (512)\n",
        "        self.vocab_size=vocab_size # Size of the vocabulary\n",
        "        self.embedding=nn.Embedding(vocab_size, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.embedding(x)*math.sqrt(self.d_model) # normalizing the variance of the embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izsEmr1RrHCb"
      },
      "source": [
        "# Positional Encoding\n",
        "\n",
        "In the original paper, the authors add the positional encodings to the input embeddings at the bottom of both the encoder and decoder block so the model can have some information about the relative or absolute position of the tokens in the sequence. The positional encodings have the same dimension $d_{model}$ as the embeddings, so that the two vectors can be summed and we can combine the semantic content from the word embeddings and positional information from the positional encodings.\n",
        "\n",
        "In the `PositionalEncoding` class below, we will create a matrix of positional encodings `pe` with dimensions `(seq_len, d_model)`. We will start by filling it with 0s. We will then apply the sine function to even indices of the positional encoding matrix while the cosine function is applied to the odd ones.\n",
        "\n",
        "$$Even Indices(2i): PE(pos,2i)=sin(\\frac{pos}{10000^{\\frac{2i}{d_{model}}}})$$\n",
        "\n",
        "$$Odd Indices(2i+1): PE(pos, 2i+1)=cos(\\frac{pos}{10000^{\\frac{2i}{d_{model}}}})$$\n",
        "\n",
        "We apply the sine and cosine functions because it allows the model to determine the position of a word based on the position of other word in the sequence, since for any fixed offset $k$, $PE_{pos+k}$ can be represented as a linear function of $PE_{pos}$. This happens due to the properties of sine and cosine functions, where a shift in the input results in a predictable change in the output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "swB2u_QIrHCc"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model:int, seq_len:int, dropout:float) -> None:\n",
        "        super().__init__()\n",
        "        self.d_model=d_model # Dimensionality of the model\n",
        "        self.seq_len=seq_len # Maximum sequence length\n",
        "        self.dropout=nn.Dropout(dropout) # dropout layer to prevent overfitting\n",
        "\n",
        "        # creating a positional ecoding matrix of shape (seq_len, d_model) filled with zeros\n",
        "        pe=torch.zeros(seq_len, d_model)\n",
        "\n",
        "        # creating a tensor representing positions (0 to seq_len -1)\n",
        "        position=torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1) # transforming `position` into a 2D tensor[seq_len,1]\n",
        "\n",
        "        # creating te division term for the positional encoding formula\n",
        "        div_term=torch.exp(torch.arange(0, d_model, 2).float()*(-math.log(10000.0)/d_model))\n",
        "\n",
        "        # apply sine to even indices in pe\n",
        "        pe[:,0::2]=torch.sin(position*div_term)\n",
        "\n",
        "        # apply cosine to odd indices in pe\n",
        "        pe[:,1::2]=torch.cos(position*div_term)\n",
        "\n",
        "        # adding an extra dimension at the beginning of pe matrix for batch handling\n",
        "        pe=pe.unsqueeze(0)\n",
        "\n",
        "        # registering 'pe' as buffer, buffer is a tensor not considered as a model parameter\n",
        "        self.register_buffer('pe',pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # adding positional encoding to the input tensor X\n",
        "        x=x+(self.pe[:,:x.shape[1],:].requires_grad_(False))\n",
        "        return self.dropout(x) # dropout for regularization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFpLY26urHCd"
      },
      "source": [
        "# Layer Normalization\n",
        "\n",
        "We have several normalization layers called `Add&Norm`.\n",
        "\n",
        "The `LayerNormalization` class below performs layer normalization on the input data. During its forward pass, we compute the mean and standard deviation of the input data. We then normalize the input data by subtracing the mean and dividing by the standard deviation plus a small number called **epsilon** to avoid any division by zero. This process results in a normalized output with a mean 0 and standard deviation 1.\n",
        "\n",
        "We will then scale the normalized output by a learnable parameter `alpha` and add a learnbale parameter called `bias`. The training process is repsonsible for adjusting these parameters. The final result is a layer-normalized tensor, which ensures that the scale of the inputs to layers in the network is consistent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_lx_pDyOrHCd"
      },
      "outputs": [],
      "source": [
        "# creating layer normalization\n",
        "class LayerNormalization(nn.Module):\n",
        "    # we define epsilon as 0.000001 to avoid division by zero\n",
        "    def __init__(self, eps: float=10**-6)-> None:\n",
        "        super().__init__()\n",
        "        self.eps=eps\n",
        "\n",
        "        # we define alpha as a trainable parameter and initialize it with ones\n",
        "        self.alpha=nn.Parameter(torch.ones(1)) # One-dimensional tensor that will be used to scale the input data\n",
        "\n",
        "        # we define bias as a trainable parameter and initialize it with zeros\n",
        "        self.bias=nn.Parameter(torch.zeros(1)) # One-dimensional tensor that will be added to the input data\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean=x.mean(dim=-1, keepdim=True) # computing the mean of the input data. Keeping the number of dimensions unchanged\n",
        "        std=x.std(dim=-1, keepdim=True) # computing the standard deviation of the input data. Keeping the number of dimensions unchanged\n",
        "\n",
        "        # returning the normalized input\n",
        "        return self.alpha*(x-mean)/(std+self.eps)+self.bias"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Upe7nDg2rHCe"
      },
      "source": [
        "# Feed-Forward Network\n",
        "\n",
        "In the fully connected feed-forward network, we apply two linear transformations with a ReLU activation in between. We can mathematically represent this operation as:\n",
        "\n",
        "$$FFN(x)=max(0, xW_{1}+b_{1})W_{2}+b_{2}$$\n",
        "\n",
        "$W_{1}$ and $W_{2}$ are the weights, while $b_{1}$ and $b_{2}$ are the biases of the two linear transformations.\n",
        "\n",
        "In the `FeedForwardBlock` below, we will define the two linear transformers -`self.linear_1` and `self.linear_2` and the inner-layer `d_ff`. The input data will first pass through the `self.linear_1` transformation, which increases its dimensionality from `d_model` to `d_ff`. The output  of this operation passes through the ReLU activation function, which introduces non-linearity so the network can learn more complex patterns, and the `self.dropout` layer is applied to mitigate overfitting. The final operation is the `self.linear_2` transformation to the dropout-modified tensor, which transforms it back to the original `d_model` dimension."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v8s1FpbnrHCf"
      },
      "outputs": [],
      "source": [
        "class FeedForwardBlock(nn.Module):\n",
        "    def __init__(self,d_model:int, d_ff:int, dropout:float) -> None:\n",
        "        super().__init__()\n",
        "        # First lienar transformation\n",
        "        self.linear_1=nn.Linear(d_model, d_ff) # W1 & b1\n",
        "        self.dropout=nn.Dropout(dropout) # Dropout to prevent overfitting\n",
        "\n",
        "        # Second linear transformation\n",
        "        self.linear_2=nn.Linear(d_ff, d_model) # W2 & b2\n",
        "\n",
        "    def forward(self, x):\n",
        "        # (batch, seq_len, d_model) --> (batch, seq_len, d_ff) --> (batch, seq_len, d_model)\n",
        "        return self.linear_2(self.dropout(torch.relu(self.linear_1(x))))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4LKZvnjXrHCg"
      },
      "source": [
        "# Multi-Head Attention\n",
        "\n",
        "The Multi-Head Attention is the most crucial component of the Transformer. It is responsible for helping the model to understand complex relationships and patterns in the data.\n",
        "\n",
        "![](https://cdn.masto.host/sigmoidsocial/media_attachments/files/111/963/621/063/011/468/original/629903a63a938f0a.png)\n",
        "\n",
        "The Multi-Head Attention block receives the input data split into queries, keys, and values organized into matrices Q, K and V. Each matrix contains different facets of the input, and they have the same dimensions as the input.\n",
        "\n",
        "![](https://hostux.social/system/media_attachments/files/111/603/992/766/474/377/small/5df72b068852f4da.webp)\n",
        "\n",
        "We then linearly transform each matrix by their respective weight matrices $W^Q$, $W^K$ and $W^V$. These transformations will result in new matrices $Q'$, $K'$ and $V'$, which will be split into smaller matrices corresponding to different heads $h$, allowing the model to attend to information from different representation subspaces in parallel. This split creates multiple sets of queries, keys, and values for each head.\n",
        "\n",
        "Finally, we concatenate every head into an H matrix, which is then transformed by another wight matrix $W^o$ to produce the multi-head attention output, a matrix $MH$ - A that retains the input dimensionality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S77OczUfrHCg"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttentionBlock(nn.Module):\n",
        "    def __init__(self, d_model: int, h:int, dropout:float)-> None: # h= number of heads\n",
        "        super().__init__()\n",
        "        self.d_model=d_model\n",
        "        self.h=h\n",
        "\n",
        "        # we ensure that the dimensions of the model is divisible by the number of heads\n",
        "        assert d_model %h==0, 'd_model is not divisible by h'\n",
        "\n",
        "        # d_k is the dimension of each attention head's key, query, and values vectors\n",
        "        self.d_k =d_model // h # d_k formula, like in the original paper\n",
        "\n",
        "        # degining the weight matrices\n",
        "        self.w_q=nn.Linear(d_model, d_model) # W_q\n",
        "        self.w_k=nn.Linear(d_model, d_model) # W_k\n",
        "        self.w_v=nn.Linear(d_model, d_model) # W_v\n",
        "        self.w_o=nn.Linear(d_model, d_model) # W_o\n",
        "\n",
        "        self.dropout=nn.Dropout(dropout) # Dropout layer to avoid overfitting\n",
        "\n",
        "    @staticmethod\n",
        "    def attention(query, key, value, mask, dropout:nn.Dropout): # mask=>when we certain words to not interact with others, we hide them\n",
        "        d_k=query.shape[-1] # the last dimension of query, key and value\n",
        "\n",
        "        # we calculate the Attention(Q,K,V) as in the formula in the image above\n",
        "        attention_scores=(query@key.transpose(-2, -1))/math.sqrt(d_k) # @=matrix multiplication sign in PyTorch\n",
        "\n",
        "        # before applying the softmax, we apply the mask to hide some interactions between words\n",
        "        if mask is not None:\n",
        "            attention_scores.masked_fill_(mask==0, -1e9) # replace each value where mask is equal to 0 by -1e9\n",
        "        attention_scores=attention_scores.softmax(dim=-1) # applying softmax\n",
        "        if dropout is not None:\n",
        "            attention_scores=dropout(attention_scores) # we apply dropout to prevent overfitting\n",
        "\n",
        "        return (attention_scores @ value), attention_scores # multiply the output matrix by the V matrix, as in the formula\n",
        "\n",
        "    def forward(self, q,k,v, mask):\n",
        "        query=self.w_q(q) # Q' matrix\n",
        "        key=self.w_k(k) # K' matrix\n",
        "        value=self.w_v(v) # V' matrix\n",
        "\n",
        "        # splitting results into smaller matrices for the different heads\n",
        "        # splitting embeddings (third dimension) into h parts\n",
        "\n",
        "        # Transpose => bring the head to the second dimension\n",
        "        query=query.view(query.shape[0], query.shape[1], self.h, self.d_k).transpose(1,2)\n",
        "\n",
        "        # Transpose => bring the head to the second dimension\n",
        "        key=key.view(key.shape[0], key.shape[1], self.h, self.d_k).transpose(1,2)\n",
        "\n",
        "        # Transpose => bring the head to the second dimension\n",
        "        value=value.view(value.shape[0], value.shape[1], self.h, self.d_k).transpose(1,2)\n",
        "\n",
        "        # obtaining the output and the attention scores\n",
        "        x, self.attention_scores=MultiHeadAttentionBlock.attention(query, key, value, mask, self.dropout)\n",
        "\n",
        "        # obtaining the H matrix\n",
        "        x=x.transpose(1,2).contiguous().view(x.shape[0], -1, self.h * self.d_k)\n",
        "\n",
        "        # multiply the H matrix by the weight matrix W_o, resulting in the MH-A matrix\n",
        "        return self.w_o(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7P6N0jLrHCh"
      },
      "source": [
        "# Residual Connection\n",
        "\n",
        "When we look at the architecture of the Transformer, we see that each sub-layer, including the `self-attention` and `Feed Forward` blocks, add its outputs to its input before passing it to the `Add&Norm` layer. This approach integrates the output with the original input in the `Add&Norm` layer. This process is known as the skip connection, which allows the Transformer to train deep networks more effectively by prociding a shotcut for the gradient to flow thorugh during backpropagation.\n",
        "\n",
        "The `ResidualConnection` class below is responsible for this process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kz2m9Gy8rHCh"
      },
      "outputs": [],
      "source": [
        "class ResidualConnection(nn.Module):\n",
        "    def __init__(self, dropout: float) -> None:\n",
        "        super().__init__()\n",
        "        # we use a dropout layer to prevent overfitting\n",
        "        self.dropout=nn.Dropout(dropout)\n",
        "        # we use a normalization layer\n",
        "        self.norm=LayerNormalization()\n",
        "\n",
        "    def forward(self, x, sublayer):\n",
        "        # we normalize the input and add it to the original input x`. This creates the residual connection process\n",
        "        return x+self.dropout(sublayer(self.norm(x)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0iKkBoxrHCh"
      },
      "source": [
        "# Encoder\n",
        "\n",
        "We wil now build the encoder. We carete the `EncoderBlock` clas, consisting of the Multi-Head Attention and Feed Forward layers, plus the residual connections. In the original paper, the Encoder Block repeats six times. We create the `Encoder` class as an assembly of multiple `EncoderBlock`s. We also add layer normalization as a final step after processing the input through all its blocks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZL6uloBFrHCi"
      },
      "outputs": [],
      "source": [
        "# building encoder block\n",
        "class EncoderBlock(nn.Module):\n",
        "    # this block takes in the MultiHeadAttentionBlock and FeedForwardBlock, as well as the dropout rate for the residual connections.\n",
        "    def __init__(self, self_attention_block: MultiHeadAttentionBlock, feed_forward_block:FeedForwardBlock, dropout: float) -> None:\n",
        "        super().__init__()\n",
        "        #Strong the self-attention block and feed-forward block\n",
        "        self.self_attention_block=self_attention_block\n",
        "        self.feed_forward_block=feed_forward_block\n",
        "        # 2 residual connections with dropout\n",
        "        self.residual_connections=nn.ModuleList([ResidualConnection(dropout) for _ in range(2)])\n",
        "\n",
        "    def forward(self, x, src_mask):\n",
        "        # Applying the first residual connection with the self-attention block\n",
        "        # Three x corresponding to query, key and value inputs plus source mask\n",
        "        x=self.residual_connections[0](x,lambda x: self.self_attention_block(x,x,x,src_mask))\n",
        "\n",
        "        # Appplying the second residual connection with the feed-forward block\n",
        "        x=self.residual_connections[1](x, self.feed_forward_block)\n",
        "\n",
        "        # output tensor after applying self-attention and feed-forward layers with residual connections\n",
        "        return x\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, layers: nn.ModuleList)-> None:\n",
        "        super().__init__()\n",
        "        self.layers=layers # storing the EncoderBlocks\n",
        "        # layer for the normalization of the output of the encoder layers\n",
        "        self.norm=LayerNormalization()\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        # Iterating over each EncoderBlock stored in self.layers\n",
        "        for layer in self.layers:\n",
        "            # Applying each EncoderBlock to the input tensot 'x'\n",
        "            x=layer(x, mask)\n",
        "        return self.norm(x) # Normalizing output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_TmnPJArHCi"
      },
      "source": [
        "# Decoder\n",
        "\n",
        "Similarly, the Decoder also consists of several DecoderBlocks that repeat six times in the original paper. The main difference it that it has an additional sub-layer that performs multi-head attention with a `cross-attention` component that uses the output of the Encoder as its keys and values while using the Decoder's input as queries. For the Output Embedding, we can use the same `InputEmbeddings` class we use for the Encoder. You can also notice that the self-attention sub-layer is masked, which restricts the model from accessing future elements in the sequence.\n",
        "\n",
        "We will start by building the `DecoderBlock` class, and then we will build the `Decoder` class, which will assemble multiple `DecoderBlock`'s."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "btSueKrMrHCi"
      },
      "outputs": [],
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "    # the DecoderBlock takes in two MultiHeadAttentionBlock. One is self-attention, while the other is cross-attention.\n",
        "    # it also takes in the feed-forward block and the dropout rate\n",
        "    def __init__(self, self_attention_block: MultiHeadAttentionBlock, cross_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float)->None:\n",
        "        super().__init__()\n",
        "        self.self_attention_block=self_attention_block\n",
        "        self.cross_attention_block=cross_attention_block\n",
        "        self.feed_forward_block=feed_forward_block\n",
        "        # list of three Residual Connection with dropout rate\n",
        "        self.residual_connections=nn.ModuleList([ResidualConnection(dropout) for _ in range(3)])\n",
        "\n",
        "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
        "        # self-attention block with query, key and value plus the target language mask\n",
        "        x=self.residual_connections[0](x, lambda x: self.self_attention_block(x,x,x, tgt_mask))\n",
        "        # the cross-attention block using two `encoder_output` for key and value plus the source language mask. It also takes in `x` for Decoder queries\n",
        "        x=self.residual_connections[1](x, lambda x: self.cross_attention_block(x, encoder_output, encoder_output, src_mask))\n",
        "\n",
        "        # feed-forward block with residual connections\n",
        "        x=self.residual_connections[2](x,self.feed_forward_block)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, layers: nn.ModuleList)-> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.layers=layers\n",
        "        self.norm=LayerNormalization()\n",
        "\n",
        "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
        "        for layer in self.layers:\n",
        "            x=layer(x, encoder_output, src_mask, tgt_mask)\n",
        "        return self.norm(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35OezZ5DrHCj"
      },
      "source": [
        "# ProjectionLayer\n",
        "\n",
        "The `ProjectionLayer` class below is responsible for converting the output of the model into a probability distribution over the `vocabulary`, where we select each output token from a vocabulary of possible tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tHcHYgmmrHCj"
      },
      "outputs": [],
      "source": [
        "class ProjectionLayer(nn.Module):\n",
        "    def __init__(self, d_model: int, vocab_size: int)-> None: # model dimension and the size of the output vocabulary\n",
        "        super().__init__()\n",
        "        # linear layer for projecting the feature space of `d_model` to the output space of `vocab_size`\n",
        "        self.proj=nn.Linear(d_model, vocab_size)\n",
        "    def forward(self, x):\n",
        "        # applying the log Softmax function to the output\n",
        "        return torch.log_softmax(self.proj(x), dim=-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FEQMVaoerHCj"
      },
      "source": [
        "# Building the Transformer\n",
        "\n",
        "We will bring together all the components of the model's architecture."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_U7No7HGrHCk"
      },
      "outputs": [],
      "source": [
        "# Creating the Transformer Architecture\n",
        "class Transformer(nn.Module):\n",
        "    # This takes in the encoder and decoder, as well the embeddings for the source and target language.\n",
        "    # It also takes in the POsitional Encoding for the source and target language, as well as the projection layer\n",
        "    def __init__(self, encoder: Encoder, decoder: Decoder, src_embed: InputEmbeddings, tgt_embed: InputEmbeddings, src_pos: PositionalEncoding, tgt_pos: PositionalEncoding, projection_layer:ProjectionLayer) -> None:\n",
        "        super().__init__()\n",
        "        self.encoder=encoder\n",
        "        self.decoder=decoder\n",
        "        self.src_embed=src_embed\n",
        "        self.tgt_embed=tgt_embed\n",
        "        self.src_pos=src_pos\n",
        "        self.tgt_pos=tgt_pos\n",
        "        self.projection_layer=projection_layer\n",
        "\n",
        "    def encode(self, src, src_mask):\n",
        "        # applying source embeddings to the input source language\n",
        "        src=self.src_embed(src)\n",
        "        # applying source positional encoding to the soruce embeddings\n",
        "        src=self.src_pos(src)\n",
        "        # returning the source embeddings plus a source mask to prevent attention to certain elements\n",
        "        return self.encoder(src, src_mask)\n",
        "\n",
        "    def decode(self, encoder_output, src_mask, tgt, tgt_mask):\n",
        "        tgt=self.tgt_embed(tgt) # applying target embeddings to the input target language (tgt)\n",
        "        tgt=self.tgt_pos(tgt) # applying target positional encoding to the target embeddings\n",
        "\n",
        "        # return the target embeddings, the output of the encoder, and both source and target masks\n",
        "        # The target mask ensures that the model won't see future elements of the sequence\n",
        "        return self.decoder(tgt, encoder_output, src_mask, tgt_mask)\n",
        "\n",
        "    # applying projection layer with the Softmax function to the Decoder output\n",
        "    def project(self, x):\n",
        "        return self.projection_layer(x)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9albL6jrHCk"
      },
      "source": [
        "# Initializing Transformer\n",
        "\n",
        "We now define a function called `build_transformer`, in which we define the parameters and everything we need to have a fully operational Transformer model for the taks of `machine translation`. And we are going to use the same parameters as in the original paper, where $d_{model}=512$, $N=6$, $h=8$, dropout rate $P_{drop}=0.1$ and $d_{ff}=2048$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5QmpfjtorHCk"
      },
      "outputs": [],
      "source": [
        "def build_transformer(src_seq_len:int, tgt_seq_len:int, d_model:int=512, N:int=6, h:int=8, dropout:float=0.1, d_ff:int=2048)->Transformer:\n",
        "    # creating embedding layers\n",
        "    tokenizer = XLNetTokenizer.from_pretrained(\"xlnet-base-cased\")\n",
        "    src_vocab_size = tokenizer.vocab_size\n",
        "    src_embed=InputEmbeddings(d_model, src_vocab_size) # source language (Source Vocabulary to 512-dimensional vectors)\n",
        "    #tgt_embed=InputEmbeddings(d_model, tgt_vocab_size) # target langauge (Target vocabulary to 512-dimensional vectors)\n",
        "\n",
        "    # creating positional encoding layers\n",
        "    src_pos=PositionalEncoding(d_model, src_seq_len, dropout)\n",
        "    tgt_pos=PositionalEncoding(d_model, tgt_seq_len, dropout)\n",
        "\n",
        "    # creating EncoderBlocks\n",
        "    encoder_blocks=[]\n",
        "    for _ in range(N):\n",
        "        encoder_self_attention_block=MultiHeadAttentionBlock(d_model, h, dropout) # self-attention\n",
        "        feed_forward_block=FeedForwardBlock(d_model, d_ff, dropout) # feedforward\n",
        "\n",
        "        # combine layers into an EncoderBlock\n",
        "        encoder_block=EncoderBlock(encoder_self_attention_block, feed_forward_block, dropout)\n",
        "        encoder_blocks.append(encoder_block) # appending EncoderBlock to the list of EncoderBlocks\n",
        "\n",
        "    # creating decoder blocks\n",
        "    decoder_blocks=[]\n",
        "    for _ in range(N):\n",
        "        decoder_self_attention_block=MultiHeadAttentionBlock(d_model, h, dropout)\n",
        "        decoder_cross_attention_block=MultiHeadAttentionBlock(d_model, h, dropout) # cross-attention\n",
        "        feed_forward_block=FeedForwardBlock(d_model, d_ff, dropout) # feedforward\n",
        "\n",
        "        # combining layers into a DecoderBlock\n",
        "        decoder_block=DecoderBlock(decoder_self_attention_block, decoder_cross_attention_block, feed_forward_block, dropout)\n",
        "        decoder_blocks.append(decoder_block) # appending DecoderBlock and DecoderBlocks lists\n",
        "\n",
        "    # creating the Encoder and Decoder by using the EncoderBlocks and DecoderBlocks lists\n",
        "    encoder=Encoder(nn.ModuleList(encoder_blocks))\n",
        "    decoder=Decoder(nn.ModuleList(decoder_blocks))\n",
        "\n",
        "    # Creating projection layer\n",
        "    projection_layer=ProjectionLayer(d_model, src_vocab_size) # map the output of Decoder to the Target Vocabulary Space\n",
        "\n",
        "    # crating the transformer by combining everything above\n",
        "    transformer=Transformer(encoder, decoder, src_embed, src_embed, src_pos, tgt_pos, projection_layer)\n",
        "\n",
        "    # Initialize the parameters\n",
        "    for p in transformer.parameters():\n",
        "        if p.dim()>1:\n",
        "            nn.init.xavier_uniform_(p)\n",
        "\n",
        "    # Assembled and initialized Transformer, Ready to be trained and validated!\n",
        "    return transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykdQfW8MrHCl"
      },
      "source": [
        "# Tokenizer\n",
        "\n",
        "Tokenization is a crucial preprocessing step for our Transformer model. In this step, we convert raw text into a number format that the model can process. There are several Tokenization strategies. We will use the `word-level` tokenization to transform each word in a sentence into a token. After tokenizing a sentence, we map each token to an unique integer ID based on the created vocabulary present in the training corpus during the training of the tokenizer. Each integer number represents a specific word in the vocabulary.\n",
        "\n",
        "Besides the words in the training corpus, Transformers use special tokens for specific purpose. These are some that we will define right away:\n",
        "\n",
        "* **[UNK]**: This token is used to identify an unknown word in the sequence\n",
        "* **[PAD]**: Padding token to ensure that all sequences in a batch have the same length, so we pad shorter sentences with this token. We use attention masks to \"tell\" the model ignore the padded tokens during training since they don't have any real meaning to the task.\n",
        "* **[SOS]**: This is a token used to signal the Start of Sentence.\n",
        "* **[EOS]**: This is a token used to signal the End of Sentence.\n",
        "\n",
        "In the `build_tokenizer` function below, we ensure a tokenizer is ready to train the model. It checks if there is an existing tokenizer, and if that is not case, it trains a new tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ATWQND16rHCl"
      },
      "outputs": [],
      "source": [
        "def get_all_sentences(ds, lang):\n",
        "    for pair in ds:\n",
        "        yield pair['translation'][lang]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oRdNwFxArHCl"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import WordLevel\n",
        "from tokenizers.trainers import WordLevelTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Defining Tokenizer\n",
        "def build_tokenizer(config, ds, lang):\n",
        "    # creating a file path for the tokenizer\n",
        "    tokenizer_path=Path(config['tokenizer_file'].format(lang))\n",
        "\n",
        "    # checking if Tokenizer already exists\n",
        "    if not Path.exists(tokenizer_path):\n",
        "        # initializing a new world-level tokenizer\n",
        "        tokenizer=Tokenizer(WordLevel(unk_token='[UNK]'))\n",
        "        tokenizer.pre_tokenizer=Whitespace() # we will split the text into tokens based on whitespace\n",
        "\n",
        "        # creating a trainer for the new tokenizer\n",
        "        trainer=WordLevelTrainer(special_tokens=['[UNK]', '[PAD]', '[SOS]','[EOS]'], min_frequency=2) # defining Word Level strategy and special tokens\n",
        "\n",
        "        # training new tokenizer on sentences from the dataset and language specified\n",
        "        tokenizer.train_from_iterator(get_all_sentences(ds, lang), trainer=trainer)\n",
        "        tokenizer.save(str(tokenizer_path)) # saving trained tokenizer to the file path specified at the beginning of the function\n",
        "    else:\n",
        "        tokenizer=Tokenizer.from_file(str(tokenizer_path)) # if the tokenizer already exist, we load it\n",
        "    return tokenizer # returns the loaded tokenizer or the trained tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0mEfVdgMrHCl"
      },
      "source": [
        "# Loading Dataset\n",
        "\n",
        "We are going to use the OpusBooks dataset. It consists of two features, `id` and `translation`. The `translation` feature contains pairs of sentences in different languages. And we will train the model on the English-Italian pair."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RO0Nx-skrHCl"
      },
      "source": [
        "We define the `casual_mask` function to create a mask for the attention machanism of the decoder. This mask prevents the model from having information about future elements in the sequence.\n",
        "\n",
        "WE start by making a square frid filled with ones. We determine the grid size with the `size` parameter. Then, we change all the numbers above the main diagonal line to zeros. Every number on one side becomes a zero,  while the rest remain ones. The function then flips all these values, turning ones into zeros and zeros into ones. This process is curcial for models that predict future tokens in a sequence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vVGdf2nwrHCm"
      },
      "outputs": [],
      "source": [
        "def casual_mask(size):\n",
        "    # creating a square matrix of dimensions 'size*size' filled with ones\n",
        "    mask=torch.triu(torch.ones(1, size, size), diagonal=1).type(torch.int)\n",
        "    return mask==0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LW9lj7hVrHCm"
      },
      "source": [
        "The `BilingualDataset` class processes the texts of the target and source langauges in the dataset by tokenizing them and adding all the necesseary special tokens. This class also certifies that the sentences are within a maximum sequence length for both languages and pads all necessray sentences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EY_XR3Ctx-Vc"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import WordLevel\n",
        "from tokenizers.trainers import WordLevelTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "#from datasets import load_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WDCdBHwv4oRF",
        "outputId": "60c87cf5-2140-4501-9d05-822a8cd30844"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Collecting nltk\n",
            "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.5)\n",
            "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nltk\n",
            "\u001b[33m  WARNING: The script nltk is installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0mSuccessfully installed nltk-3.9.1\n"
          ]
        }
      ],
      "source": [
        "pip install --user -U nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zCpw2v8YrHCn"
      },
      "source": [
        "The `get_ds` function id defined to load and prepare the dataset for training and validaton. In this function, we build or load the tokenizer, split the dataset, and create DataLoaders, so the model can successfully iterate over the dataset in batches. The result of these functions is tokenizers for the source and taret languages plus the DataLoader objects."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VM63FzjLul4W",
        "outputId": "0f98e29e-5cc2-4a78-afab-7ec4a51e67f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/527.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m527.3/527.3 kB\u001b[0m \u001b[31m35.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 17.0.0 which is incompatible.\n",
            "ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 17.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Enter your token (input will not be visible): \n",
            "Add token as git credential? (Y/n) y\n",
            "Token is valid (permission: read).\n",
            "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
            "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
            "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
            "\n",
            "git config --global credential.helper store\n",
            "\n",
            "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n",
            "Token has not been saved to git credential helper.\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ],
      "source": [
        "!pip install -q datasets\n",
        "!huggingface-cli login\n",
        "token = \"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\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1FOoaKGMeWve",
        "outputId": "3ec7aabc-9dc8-4700-afbd-68266656fce8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.10/dist-packages (17.0.0)\n",
            "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.10/dist-packages (from pyarrow) (1.26.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade pyarrow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 971
        },
        "id": "UBCpV2y3fC-s",
        "outputId": "a3c6f1e0-deeb-4a7a-c021-8dbf4a507cb9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found existing installation: pyarrow 17.0.0\n",
            "Uninstalling pyarrow-17.0.0:\n",
            "  Successfully uninstalled pyarrow-17.0.0\n",
            "Found existing installation: datasets 2.21.0\n",
            "Uninstalling datasets-2.21.0:\n",
            "  Successfully uninstalled datasets-2.21.0\n",
            "Collecting pyarrow\n",
            "  Using cached pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
            "Collecting datasets\n",
            "  Using cached datasets-2.21.0-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.10/dist-packages (from pyarrow) (1.26.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.15.4)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.1.4)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.5)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.7.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Using cached pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (39.9 MB)\n",
            "Using cached datasets-2.21.0-py3-none-any.whl (527 kB)\n",
            "Installing collected packages: pyarrow, datasets\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 17.0.0 which is incompatible.\n",
            "ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 17.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-2.21.0 pyarrow-17.0.0\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "6be111b8615c4dd293638f71d0ad3cc2",
              "pip_warning": {
                "packages": [
                  "datasets",
                  "pyarrow"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip uninstall -y pyarrow datasets\n",
        "!pip install pyarrow datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "30H9LtIReru-"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load the dataset\n",
        "dataset = load_dataset('Amod/mental_health_counseling_conversations')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0quk5kDIvm-_"
      },
      "outputs": [],
      "source": [
        "dataset = dataset['train'].shuffle(seed=1).select(range(3000))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jnc_sPmbdja1"
      },
      "outputs": [],
      "source": [
        "test = []\n",
        "train = []\n",
        "for pair in dataset:\n",
        "  if len(pair['Context']) > 1000 or len(pair['Response']) > 1000:\n",
        "    test.append(pair)\n",
        "  else:\n",
        "    train.append(pair)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sdpfSqYfh0Fs",
        "outputId": "501e5d51-aeba-4eab-ac42-660b83febd02"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1812\n"
          ]
        }
      ],
      "source": [
        "print(len(train))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sGr-twHjiJiH",
        "outputId": "1a9892ba-7521-47d3-d3e6-1b33cc3f7cce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "154\n",
            "362\n"
          ]
        }
      ],
      "source": [
        "print(len(train[2]['Context']))\n",
        "print(len(train[2]['Response']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 149,
          "referenced_widgets": [
            "c5bc0753284e4b74be15790051ef5740",
            "18f1e1e2ae5d4e0287650abb8b4125db",
            "6fa1028fd9594bcd9a9b9951d7c1203c",
            "fb53c2b4f9e944ea9d8b94448d8bf912",
            "cb51e462465e4cb0af34588aecde59b8",
            "061e98be0cd5441aa802a9f75944b0ab",
            "6ef47a1feae34ea1b48affab3ddef4c4",
            "6e79fe118e924433b011dbc6f113bc6e",
            "a77e0776db544ee0a55cfcc47e338832",
            "0c05d14ccb6b4055baafff9b8a831ae7",
            "20556a94f21b4c9281980551aeb89e1f",
            "dd6d928046a14faf960dd1c27c795099",
            "95b361b20fa044348b6504109378e417",
            "319c2b9080dc4df48dc145b4018b9dd3",
            "f542ba87319644d9860d81569626fd7a",
            "6f9876bd1d21498095f87978b6da2880",
            "3ce0758821324f38af3a5d4ae881b2da",
            "81798348ac8a46faaa2f53acecef2167",
            "b32330d1558548e39e0dd7db8ad65b7b",
            "0a6d46f7ece84a7d80e5570124c23c70",
            "7ca5ffbaaac6458c822acfc2780b3772",
            "cbbef773527b48c5946dd104c445fc0b",
            "3462b7bd69484e75a37bb03262dd2f2b",
            "f6b514f0605a4751b0446aa4a41b2d09",
            "43c80a5179934407b25b5e850a48ddc4",
            "07b36552dbf44306b359fe2e4271c774",
            "41b378532d2045ae839265b74fbbfa56",
            "8e2b9c432e6e4973b5502c4893283e34",
            "74f802adf8b449d4a93af5dd49f3c4d5",
            "b39408c90b9a4713bfe92846c75bf230",
            "9693a665ff63483785cdc5a9737cca01",
            "66f32025acfc48b3b5e2736e6b0d6693",
            "40f333864302417d96962b7de7b55524"
          ]
        },
        "id": "jBK85XqAjCsS",
        "outputId": "82c37bb3-2ffb-4b63-d250-ead1b7fe9af9"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "spiece.model:   0%|          | 0.00/798k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c5bc0753284e4b74be15790051ef5740"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.38M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dd6d928046a14faf960dd1c27c795099"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/760 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3462b7bd69484e75a37bb03262dd2f2b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max length of Context sentence: 39\n",
            "Max length of Response sentence: 56\n"
          ]
        }
      ],
      "source": [
        "for pair in train:\n",
        "\n",
        "        max_len_src=0\n",
        "        max_len_tgt=0\n",
        "        from transformers import XLNetTokenizer\n",
        "        tokenizer = XLNetTokenizer.from_pretrained(\"xlnet-base-cased\")\n",
        "        src_ids=tokenizer.tokenize(pair['Context'])\n",
        "        tgt_ids=tokenizer.tokenize(pair['Response'])\n",
        "\n",
        "        src_ids = tokenizer.convert_tokens_to_ids(src_ids)\n",
        "        tgt_ids = tokenizer.convert_tokens_to_ids(tgt_ids)\n",
        "\n",
        "        max_len_src=max(max_len_src, len(src_ids))\n",
        "        max_len_tgt=max(max_len_tgt, len(tgt_ids))\n",
        "\n",
        "print(f'Max length of Context sentence: {max_len_src}')\n",
        "print(f'Max length of Response sentence: {max_len_tgt}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "referenced_widgets": [
            "eb5612bebe904e988fc68c3ab27390f6",
            "c410858f1a1b4bc686d9d07f31f1cb87",
            "515b5a7d234d42a68f608fbb0700d16b",
            "10c5352158ab41b6a7cd7bbe1f20f4a0"
          ]
        },
        "id": "kXk1UFxk6ctw",
        "outputId": "c1cc1bbd-349b-4948-a43e-cfbfb0c88985"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "eb5612bebe904e988fc68c3ab27390f6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c410858f1a1b4bc686d9d07f31f1cb87",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "515b5a7d234d42a68f608fbb0700d16b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "10c5352158ab41b6a7cd7bbe1f20f4a0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Load a pre-trained tokenizer (for example, BERT's tokenizer)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# Example sentence\n",
        "sentence = \"How can I tokenize a sentence?\"\n",
        "\n",
        "# Tokenize the sentence\n",
        "tokens = tokenizer.tokenize(sentence)\n",
        "\n",
        "# Convert tokens to token IDs (numbers)\n",
        "token_ids = tokenizer.convert_tokens_to_ids(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Q7P2xmKH6hR7",
        "outputId": "d9a9e7b6-430c-4386-ff77-86e2d7e0a8e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "How can I tokenize a sentence?\n",
            "['how', 'can', 'i', 'token', '##ize', 'a', 'sentence', '?']\n",
            "[2129, 2064, 1045, 19204, 4697, 1037, 6251, 1029]\n"
          ]
        }
      ],
      "source": [
        "print(sentence)\n",
        "print(tokens)\n",
        "print(token_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "u7Q0qb_j68W-",
        "outputId": "4a0f6170-8d3b-4839-a301-0c83e569c46b"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "invalid literal for int() with base 10: '['",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-69-7ac07f5dcede>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_ids_to_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[EOS]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_fast.py\u001b[0m in \u001b[0;36mconvert_ids_to_tokens\u001b[0;34m(self, ids, skip_special_tokens)\u001b[0m\n\u001b[1;32m    407\u001b[0m         \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m             \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    410\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mskip_special_tokens\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_special_ids\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: '['"
          ]
        }
      ],
      "source": [
        "print(tokenizer.convert_ids_to_tokens(tokenizer.tokenize(\"[EOS]\")))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "NY4g6lvcvt2S"
      },
      "outputs": [],
      "source": [
        "print(dataset['Context'][0])\n",
        "\n",
        "import nltk\n",
        "print(WordLevel.from_file(dataset['Context'][0]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "h_eqOQPfvzte"
      },
      "outputs": [],
      "source": [
        "print(dataset['Response'][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "GD3DwPpzDTHf"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download NLTK resources (run this once)\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Define a sample sentence\n",
        "sentence = \"How can I tokenize a sentence using NLTK?\"\n",
        "\n",
        "# Tokenize the sentence using NLTK\n",
        "tokens = word_tokenize(sentence)\n",
        "\n",
        "# Manually add the special tokens [SOS] and [EOS]\n",
        "tokens_with_special_tokens = ['[SOS]'] + tokens + ['[EOS]']\n",
        "\n",
        "# Print the tokenized sentence with special tokens\n",
        "print(\"Tokens:\", tokens_with_special_tokens)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "JK6fwvA5DYcj"
      },
      "outputs": [],
      "source": [
        "# Define a simple vocabulary mapping (this is usually done by a tokenizer model)\n",
        "vocab = {word: idx for idx, word in enumerate(set(tokens_with_special_tokens))}\n",
        "vocab['[SOS]'] = len(vocab)\n",
        "vocab['[EOS]'] = len(vocab) + 1\n",
        "\n",
        "# Convert tokens to IDs using the vocabulary\n",
        "token_ids = [vocab[token] for token in tokens_with_special_tokens]\n",
        "\n",
        "print(\"Vocabulary:\", vocab)\n",
        "print(\"Token IDs:\", token_ids)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "g9u1kaDR8kzI"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "# Load a specific tokenizer (e.g., BERT's tokenizer)\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# Example sentence\n",
        "sentence = \"How can I tokenize a sentence?\"\n",
        "\n",
        "# Step 1: Manually apply the basic tokenizer (this handles splitting into words, punctuation, etc.)\n",
        "basic_tokens = tokenizer.basic_tokenizer.tokenize(sentence)\n",
        "print(\"Basic Tokens:\", basic_tokens)\n",
        "\n",
        "# Step 2: Manually apply the WordPiece tokenizer (this breaks words into subwords)\n",
        "wordpiece_tokens = []\n",
        "for token in basic_tokens:\n",
        "    wordpiece_tokens.extend(tokenizer.wordpiece_tokenizer.tokenize(token))\n",
        "print(\"WordPiece Tokens:\", wordpiece_tokens)\n",
        "\n",
        "# Step 3: Convert tokens into token IDs\n",
        "token_ids = tokenizer.convert_tokens_to_ids([\"[s]\"] + wordpiece_tokens + [\"[/s]\"])\n",
        "print(\"Token IDs:\", token_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "8vpe7aje_m5k"
      },
      "outputs": [],
      "source": [
        "from transformers import GPT2Tokenizer\n",
        "\n",
        "# Load GPT-2 tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "# Example sentence\n",
        "sentence = \"[s] How can I tokenize a sentence? [/s]\"\n",
        "\n",
        "# Tokenize and convert to token IDs\n",
        "tokens = tokenizer.tokenize(sentence)\n",
        "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "print(\"Tokens:\", tokens)\n",
        "print(\"Token IDs:\", token_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "d2vXHXACAHcx"
      },
      "outputs": [],
      "source": [
        "from transformers import RobertaTokenizer\n",
        "\n",
        "# Load RoBERTa tokenizer\n",
        "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
        "\n",
        "# Example sentence\n",
        "sentence = \"How can I tokenize a sentence?\"\n",
        "\n",
        "# Tokenize and convert to token IDs\n",
        "tokens = tokenizer.tokenize(sentence)\n",
        "token_ids = tokenizer.convert_tokens_to_ids([\"[s]\"] + tokens + [\"[/s]\"])\n",
        "\n",
        "print(\"Tokens:\", tokens)\n",
        "print(\"Token IDs:\", token_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "-M855z44ASvY"
      },
      "outputs": [],
      "source": [
        "from transformers import XLNetTokenizer\n",
        "\n",
        "# Load XLNet tokenizer\n",
        "tokenizer = XLNetTokenizer.from_pretrained(\"xlnet-base-cased\")\n",
        "\n",
        "# Example sentence\n",
        "sentence = \"How can I tokenize a sentence?\"\n",
        "\n",
        "# Tokenize and convert to token IDs\n",
        "tokens = tokenizer.tokenize(sentence)\n",
        "token_ids = tokenizer.convert_tokens_to_ids([\"<s>\"] + tokens + [\"</s>\"] + [\"<pad>\"])\n",
        "\n",
        "print(\"Tokens:\", tokens)\n",
        "print(\"Token IDs:\", token_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Yxop-hHsNlG7"
      },
      "outputs": [],
      "source": [
        "from transformers import T5Tokenizer\n",
        "\n",
        "# Load T5 tokenizer\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"xlnet-base-cased\")\n",
        "\n",
        "# Example token ID\n",
        "token_id = 5  # For example, ID for '<pad>' (Padding Token)\n",
        "\n",
        "# Convert token ID to token\n",
        "token = tokenizer.convert_ids_to_tokens(token_id)\n",
        "print(\"Token:\", token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "719gVZcgAbCn"
      },
      "outputs": [],
      "source": [
        "from transformers import T5Tokenizer\n",
        "\n",
        "# Load T5 tokenizer\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
        "\n",
        "# Example sentence\n",
        "sentence = \"How can I tokenize a sentence?\"\n",
        "\n",
        "# Tokenize and convert to token IDs\n",
        "tokens = tokenizer.tokenize(sentence)\n",
        "'''start_tokens = tokenizer.tokenize(\"<s>\")\n",
        "end_tokens = tokenizer.tokenize(\"</s>\")'''\n",
        "start_token_ids = tokenizer.convert_tokens_to_ids([\"<s>\"])\n",
        "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "end_token_ids = tokenizer.convert_tokens_to_ids([\"</s>\"])\n",
        "start_torch_token_ids = torch.tensor(start_token_ids, dtype=torch.int64)\n",
        "torch_token_ids = torch.tensor(token_ids, dtype=torch.int64)\n",
        "end_torch_token_ids = torch.tensor(end_token_ids, dtype=torch.int64)\n",
        "torch_cat = torch.cat((start_torch_token_ids, torch_token_ids, end_torch_token_ids))\n",
        "\n",
        "print(\"Tokens:\", tokens)\n",
        "print(\"Token IDs:\", token_ids)\n",
        "print(start_torch_token_ids)\n",
        "print(end_torch_token_ids)\n",
        "print(torch_cat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aQV5JJgArHCm"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "from transformers import XLNetTokenizer\n",
        "import torch\n",
        "\n",
        "class Counselling_doc(Dataset):\n",
        "    # This takes in the dataset containing sentence pairs, the tokenizers for target and source language, and the strings of souce and target languages\n",
        "    # `seq_len` defines the sequence length for both languages\n",
        "    def __init__(self, ds, tokenizer, seq_len) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.seq_len=seq_len\n",
        "        self.ds=ds\n",
        "        self.tokenizer_src=tokenizer\n",
        "        self.src_lang='Context'\n",
        "        self.tgt_lang='Response'\n",
        "\n",
        "        # defining special tokens by using the targte language tokenizer\n",
        "        self.sos_token=torch.tensor(self.tokenizer_src.convert_tokens_to_ids([\"<s>\"]), dtype = torch.int64)\n",
        "        self.eos_token=torch.tensor(self.tokenizer_src.convert_tokens_to_ids([\"</s>\"]), dtype = torch.int64)\n",
        "        self.pad_token=torch.tensor(self.tokenizer_src.convert_tokens_to_ids([\"<pad>\"]), dtype = torch.int64)\n",
        "\n",
        "    # Total number os instances in the dataset (some pairs are larger than others)\n",
        "    def __len__(self):\n",
        "        return len(self.ds)\n",
        "\n",
        "    # using the index to retrive source and target texts\n",
        "    def __getitem__(self, index):\n",
        "        src_target_pair=self.ds[index]\n",
        "        src_text=src_target_pair[self.src_lang]\n",
        "        tgt_text=src_target_pair[self.tgt_lang]\n",
        "\n",
        "        # tokenizing source and target texts\n",
        "        #enc_input_tokens=self.tokenizer_src.encode(src_text).ids\n",
        "        #dec_input_tokens=self.tokenizer_src.encode(tgt_text).ids\n",
        "\n",
        "        enc_input_tokens=self.tokenizer_src.tokenize(src_text)\n",
        "        dec_input_tokens=self.tokenizer_src.tokenize(tgt_text)\n",
        "\n",
        "\n",
        "        enc_input_tokens=torch.tensor(self.tokenizer_src.convert_tokens_to_ids(enc_input_tokens), dtype = torch.int64)\n",
        "        dec_input_tokens=torch.tensor(self.tokenizer_src.convert_tokens_to_ids(dec_input_tokens), dtype = torch.int64)\n",
        "\n",
        "\n",
        "        # computing how many padding tokens need to be added to the tokenized texts source tokens\n",
        "        enc_num_padding_tokens=self.seq_len-len(enc_input_tokens)-2 # subtracting the two '[EOS]' and '[SOS]' special tokens\n",
        "\n",
        "        # target tokens\n",
        "        dec_num_padding_tokens=self.seq_len-len(dec_input_tokens)-1 # subtracting the '[SOS]' special token\n",
        "\n",
        "        # If the texts exceed the 'seq_len' allowed, it will raise an error. This means that one of the sentences in the pair is too long to be processed\n",
        "        # given the current sequence length limit(this will be defined in the config dictionary below)\n",
        "        if enc_num_padding_tokens<0 or dec_num_padding_tokens<0:\n",
        "            raise ValueError('Sentence is too long')\n",
        "\n",
        "        # building the encoder input tensor by combining several elements\n",
        "        encoder_input=torch.cat([\n",
        "            self.sos_token, # inserting the '[SOS]' token\n",
        "            enc_input_tokens, # inserting the tokenized source text,\n",
        "            self.eos_token, # inserting the '[EOS]' token\n",
        "            torch.tensor([self.pad_token] * enc_num_padding_tokens, dtype=torch.int64) # adding padding tokens\n",
        "        ])\n",
        "\n",
        "        # building the decoder input tensor by combining several elements\n",
        "        decoder_input=torch.cat([\n",
        "            self.sos_token, # inserting the '[SOS]' token\n",
        "            dec_input_tokens, # indersting the tokenized target text\n",
        "            torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype=torch.int64) # adding padding tokens\n",
        "        ])\n",
        "\n",
        "        # creating a label tensor, the expected output for training the model\n",
        "        label=torch.cat([\n",
        "            dec_input_tokens, # inserting the tokenized targate text\n",
        "            self.eos_token, # inserting the '[EOS]' token\n",
        "            torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype=torch.int64) # adding padding tokens\n",
        "        ])\n",
        "\n",
        "        # Ensuring that the length of each tensor above is equal to the defined `seq_len`\n",
        "        assert encoder_input.size(0)==self.seq_len\n",
        "        assert decoder_input.size(0)==self.seq_len\n",
        "        assert label.size(0)==self.seq_len\n",
        "        \"& casual_mask(decoder_input.size(0)\"\n",
        "        return {\n",
        "            'encoder_input': encoder_input,\n",
        "            'decoder_input': decoder_input,\n",
        "            'encoder_mask': (encoder_input!=self.pad_token).unsqueeze(0).unsqueeze(0).int(),\n",
        "            'decoder_mask': (decoder_input!=self.pad_token).unsqueeze(0).unsqueeze(0).int() & casual_mask(decoder_input.size(0)),\n",
        "            'label':label,\n",
        "            'src_text': src_text,\n",
        "            'tgt_text': tgt_text\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "T-Nv8kP8Rjkj"
      },
      "outputs": [],
      "source": [
        "start = Counselling_doc(dataset, XLNetTokenizer.from_pretrained(\"xlnet-base-cased\"), 'Context', 'Response', 512)\n",
        "start.__getitem__(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PTX0KxylrHCn"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "\n",
        "def get_ds(dataset):\n",
        "    # the language pairs will be defined in the 'config' dictionary we will build later\n",
        "    #ds_raw=load_dataset('opus_books', f'{config[\"lang_src\"]}-{config[\"lang_tgt\"]}', split='train')\n",
        "\n",
        "    # building or loading tokenizer for both the source and target languages\n",
        "    #tokenizer_src=build_tokenizer(config, ds_raw, config['lang_src'])\n",
        "    #tokenizer_tgt=build_tokenizer(config, ds_raw, config['lang_tgt'])\n",
        "\n",
        "    # splitting the dataset for training and validation\n",
        "    #train_ds_size=int(0.9*len(ds_raw)) #90% for training\n",
        "    #val_ds_size=len(ds_raw)-train_ds_size # 10% for validation\n",
        "    #train_ds_raw, val_ds_raw=random_split(ds_raw, [train_ds_size, val_ds_size]) # randomly splitting the dataset\n",
        "\n",
        "    # processing data with the BilingualDataset class\n",
        "    tokenizer = XLNetTokenizer.from_pretrained(\"xlnet-base-cased\")\n",
        "    train_ds = Counselling_doc(train, tokenizer, 350)\n",
        "    #val_ds=BilingualDataset(val_ds_raw, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])\n",
        "\n",
        "    # iterating over the entire dataset and printing the maximum length found in the sentences of both the source and target languages\n",
        "    max_len_src=0\n",
        "    max_len_tgt=0\n",
        "\n",
        "    for pair in dataset:\n",
        "        src_ids=tokenizer.tokenize(pair['Context'])\n",
        "        tgt_ids=tokenizer.tokenize(pair['Response'])\n",
        "\n",
        "        src_ids = tokenizer.convert_tokens_to_ids(src_ids)\n",
        "        tgt_ids = tokenizer.convert_tokens_to_ids(tgt_ids)\n",
        "\n",
        "        max_len_src=max(max_len_src, len(src_ids))\n",
        "        max_len_tgt=max(max_len_tgt, len(tgt_ids))\n",
        "\n",
        "    print(f'Max length of Context sentence: {max_len_src}')\n",
        "    print(f'Max length of Response sentence: {max_len_tgt}')\n",
        "\n",
        "    # creating dataloaders for the training and validation sets\n",
        "    # Dataloaders are used to iterate over the dataset in batches during training and validation\n",
        "    train_dataloader=DataLoader(train_ds, batch_size=8, shuffle=True) # Batch size will be defined in the config dictionary\n",
        "    #val_dataloader=DataLoader(val_ds, batch_size=1, shuffle=True)\n",
        "\n",
        "\n",
        "    return train_dataloader, tokenizer # returning the dataloader objects and tokenizers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "draKWyd-94wz"
      },
      "outputs": [],
      "source": [
        "for pair in dataset:\n",
        "    src_ids=tokenizer.tokenize(pair['Context'])\n",
        "    tgt_ids=tokenizer.tokenize(pair['Response'])\n",
        "\n",
        "    print(src_ids)\n",
        "    print(tgt_ids)\n",
        "    tokenizer = XLNetTokenizer.from_pretrained(\"xlnet-base-cased\")\n",
        "    #dick = tokenizer.tokenize(src_ids)\n",
        "    #print(dick)\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "6WK5UnS87JQd"
      },
      "outputs": [],
      "source": [
        "train, token = get_ds(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "WmQEArpHTvJZ"
      },
      "outputs": [],
      "source": [
        "from transformers import XLNetTokenizer\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "\n",
        "tokenizer = XLNetTokenizer.from_pretrained(\"xlnet-base-cased\")\n",
        "vocab_size = tokenizer.vocab_size\n",
        "print(vocab_size)\n",
        "'''train_ds = Counselling_doc(dataset, tokenizer, 512)\n",
        "train_dataloader=DataLoader(train_ds, batch_size=8, shuffle=True)\n",
        "print(3)'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "5iOEo0Ns6qCH"
      },
      "outputs": [],
      "source": [
        "print(train_dataloader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sa9upkdXrHCn"
      },
      "source": [
        "# Validation Loop\n",
        "\n",
        "We will now create two functions for the validation loop. The validation loop is crucial to evaluate model performance in translating sentences from data it has not seen during training. We will define two functions. The first function, `greedy_decode`, gives us the model's output by obtaining the most probable next token. The second function, `run_validation` is responsible for running the validation process in which we decide the model's output and compare it with the reference text for the target sentence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "vcds79lBrHCo"
      },
      "outputs": [],
      "source": [
        "def greedy_decode(model, source, source_mask, tokenizer_src, tokenizer_tgt, max_len, device):\n",
        "    # retrieving the indices from the start and end of sequences of the target tokens\n",
        "    sos_idx=tokenizer_tgt.token_to_id('[SOS]')\n",
        "    eos_idx=tokenizer_tgt.token_to_id('[EOS]')\n",
        "\n",
        "    # computing the output of the encoder for the source sequence\n",
        "    encoder_output=model.encode(source, source_mask)\n",
        "    # initializing the decoder input with the Start of Sentence token\n",
        "    decoder_input=torch.empty(1,1).fill_(sos_idx).type_as(source).to(device)\n",
        "\n",
        "    # looping until the `max_len`, maximum length is reached\n",
        "    while True:\n",
        "        if decoder_input.size(1)==max_len:\n",
        "            break\n",
        "\n",
        "        # building a mask for decoder input\n",
        "        decoder_mask=casual_mask(decoder_input.size(1)).type_as(source_mask).to(device)\n",
        "\n",
        "        # calculating the output of the decoder\n",
        "        out=model.decode(encoder_output, source_mask, decoder_input, decoder_mask)\n",
        "\n",
        "        # applying the projection layer to get the probabilities for the next token\n",
        "        prob=model.project(out[:,-1])\n",
        "\n",
        "        # selecting token with the highest probability\n",
        "        _, next_word=torch.max(prob, dim=1)\n",
        "        decoder_input=torch.cat([decoder_input, torch.empty(1,1).type_as(source).fill_(next_word.item()).to(device)], dim=1)\n",
        "\n",
        "        # if the next token is an End of sentence token, we finish the loop\n",
        "        if next_word==eos_idx:\n",
        "            break\n",
        "\n",
        "    # sequence of tokens generated by the decoder\n",
        "    return decoder_input.squeeze(0)\n",
        "\n",
        "\n",
        "# defining function to evaluate the model on the validation dataset, num_examples=2, two examples per run\n",
        "def run_validation(model, validation_ds, tokenizer_src, tokenizer_tgt, max_len, device, print_msg, global_state, writer, num_examples=2):\n",
        "    model.eval()\n",
        "    count=0 # initializing counter to keep track of how many examples have been processed\n",
        "\n",
        "    console_width=80 # fixed width for printed messages\n",
        "\n",
        "    # creating evaluation loop\n",
        "    with torch.no_grad(): # ensuring that no gradients are computed during this process\n",
        "        for batch in validation_ds:\n",
        "            count+=1\n",
        "            encoder_input=batch['encoder_input'].to(device)\n",
        "            encoder_mask=batch['encoder_mask'].to(device)\n",
        "\n",
        "            # ensuring that the batch_size of the validation set is 1\n",
        "            assert encoder_input.size(0)==1, 'Batch size must be 1 for validation.'\n",
        "\n",
        "            # applying the `greedy_decode` functio to get the model's output of the source text of the input batch\n",
        "            model_out=greedy_decode(model, encoder_input, encoder_mask, tokenizer_src, tokenizer_tgt, max_len, device)\n",
        "\n",
        "            # retraeving source and target texts from the batch\n",
        "            source_text=batch['src_text'][0]\n",
        "            target_text=batch['tgt_text'][0] # true translation\n",
        "            model_out_text=tokenizer_tgt.decode(model_out.detach().cpu().numpy()) # decoded, human-readable model ouptut\n",
        "\n",
        "            # printing results\n",
        "            print_msg('-'*console_width)\n",
        "            print_msg(f'SOURCE: {source_text}')\n",
        "            print_msg(f'TARGET: {target_text}')\n",
        "            print_msg(f'PREDICTED: {model_out_text}')\n",
        "\n",
        "            # After two examples, we break the loop\n",
        "            if count==num_examples:\n",
        "                break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pi_yPumNrHCo"
      },
      "source": [
        "# Training Loop\n",
        "\n",
        "We first start by defining the `get_model` function to load the model by calling the `build_transformer` function we have previously defined. This function uses the `config` dictionary to set a few parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kssn9gTorHCo"
      },
      "outputs": [],
      "source": [
        "# we pass as parameters the config dictionary, the length of the vocabulary of the source language and the target language\n",
        "def get_model(config):\n",
        "    # loading model using the `build_transformer` function\n",
        "    # we will use the lengths of the source language and atarget language vocabularies, the `seq_len`, and the dimensionality of embeddings\n",
        "    model=build_transformer(config['seq_len'], config['seq_len'], config['d_model'])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Z7PiIIErHCp"
      },
      "source": [
        "We will define two functions to configure our model and the training process. In the `get_config` function, we define crucial parameters for the training process. `batch_size` for the number of training examples used in one iteration, `num_epochs` can as the number of times the entire dataset is passed forward and backward through the Transformer, `lr` as the learning rate for the optimizer, etc. We will also finally define the pairs from the OpusBook dataset, `lang_src:en` for selecting English as the source langauge and `lang_tgt:it` for selecting italian as the target language.\n",
        "\n",
        "The `get_weights_file_path` function constructs the file path for saving or loading model weights for any specific epoch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RGwrJgT7rHCp"
      },
      "outputs": [],
      "source": [
        "# define settings for building and training the transfomer model\n",
        "def get_config():\n",
        "    return{\n",
        "        'batch_size':32,\n",
        "        'num_epochs':2500,\n",
        "        'lr':10**-4,\n",
        "        'seq_len':1028,\n",
        "        'd_model': 512, # dimensions of the embeddings in the transformer. 512 like in the paper\n",
        "        'lang_src':'Context',\n",
        "        'lang_tgt':'Response',\n",
        "        'model_folder': '/content/weights',\n",
        "        'model_basename':'tmodel_',\n",
        "        'preload': None,\n",
        "        'tokenizer_file': 'tokenizer_{0}.json',\n",
        "        'experiment_name':'runs/tmodel'\n",
        "    }\n",
        "\n",
        "# function to construct the path for saving and retrieving model weights\n",
        "def get_weights_file_path(config, epoch: str):\n",
        "    model_folder=config['model_folder'] # extracting model folder from the config\n",
        "    model_basename=config['model_basename'] # extracting the base name for model files\n",
        "    model_filename=f'{model_basename}{epoch}.pt'\n",
        "    return str(Path('.')/model_folder/model_filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHKGCVUIrHCz"
      },
      "source": [
        "We will define the `train_model` function, which takes the `config` argument as input. In this function, we will load the model and its necessary components onto the GPU for faster training, set the `Adam` optimizer, and configure the `CrossEntropyLoss` function to compute the differences between the translations output by the model and the reference translations from the dataset. Every loop necessary for iterating over the training batches, performing backpropagation, and computing the gradients it in this function. We will also use it to run the validation function and save the current state of the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_CkRLPCPrHCz"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from pathlib import Path\n",
        "\n",
        "def train_model(config):\n",
        "    device=(torch.device('cuda') if torch.cuda.is_available() else 'cpu')\n",
        "    print(f'Using. device {device}')\n",
        "\n",
        "    # creating model directory to store weights\n",
        "    Path(config['model_folder']).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    train_dataloader, tokenizer = get_ds(train)\n",
        "\n",
        "    tokenizer = XLNetTokenizer.from_pretrained(\"xlnet-base-cased\")\n",
        "    vocab_size = tokenizer.vocab_size\n",
        "\n",
        "    # loading model\n",
        "    model=get_model(config).to(device)\n",
        "\n",
        "    # Tensorboard\n",
        "    writer=SummaryWriter(config['experiment_name'])\n",
        "\n",
        "    # setting up the Adam optimizer with the specified leanring rate from the `config` dictionary plus an epsilon value\n",
        "    optimizer=torch.optim.Adam(model.parameters(), lr=config['lr'], eps=1e-9)\n",
        "\n",
        "    # initializing epoch and global step variables\n",
        "    initial_epoch=0\n",
        "    global_step=0\n",
        "\n",
        "    # checking if there is a pre-trained model to load\n",
        "    if config['preload']:\n",
        "        model_filename=get_weights_file_path(config, config['preload'])\n",
        "        print(f'Preloading model {model_filename}')\n",
        "\n",
        "        state=torch.load(model_filename)\n",
        "\n",
        "        # sets epoch to the saved in the state plus one, to resume from wher it stopped\n",
        "        initial_epoch=state['epoch']+1\n",
        "        # loading the optimizer state from the saved model\n",
        "        optimizer.load_state_dict(state['optimizer_state_dict'])\n",
        "        global_step=state['global_step']\n",
        "\n",
        "    # initializing CrossEntropyLoss function for training\n",
        "    # we ignore padding tokens when computing loss, as they are not relevant for the learning process\n",
        "    # we also apply label_smoothing to prevent overfitting\n",
        "    loss_fn=nn.CrossEntropyLoss(ignore_index=tokenizer.convert_tokens_to_ids('<pad>'), label_smoothing=0.1).to(device)\n",
        "\n",
        "    for epoch in range(initial_epoch, config['num_epochs']):\n",
        "        batch_iterator=tqdm(train_dataloader, desc=f'Professing epoch {epoch:02d}')\n",
        "\n",
        "        for batch in batch_iterator:\n",
        "            model.train()\n",
        "\n",
        "            # loading input data and masks onto the GPU\n",
        "            encoder_input=batch['encoder_input'].to(device)\n",
        "            decoder_input=batch['decoder_input'].to(device)\n",
        "            encoder_mask=batch['encoder_mask'].to(device)\n",
        "            decoder_mask=batch['decoder_mask'].to(device)\n",
        "\n",
        "            # runing tensors through the transformer\n",
        "            encoder_output=model.encode(encoder_input, encoder_mask)\n",
        "            decoder_output=model.decode(encoder_output, encoder_mask, decoder_input, decoder_mask)\n",
        "            proj_output=model.project(decoder_output)\n",
        "\n",
        "            # loading the target labels onto the GPU\n",
        "            label=batch['label'].to(device)\n",
        "\n",
        "            # computing loss between model's output and true labels\n",
        "            loss=loss_fn(proj_output.view(-1, vocab_size), label.view(-1))\n",
        "\n",
        "            # updating progress bar\n",
        "            batch_iterator.set_postfix({f'loss':f'{loss.item():6.3f}'})\n",
        "\n",
        "            writer.add_scalar('train loss', loss.item(), global_step)\n",
        "            writer.flush()\n",
        "\n",
        "            # performing backpropagation\n",
        "            loss.backward()\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            # clearing the gradients to prepare for the next bacth\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            global_step+=1 # updating global step count\n",
        "\n",
        "\n",
        "# we run the 'run_validation' function at the end of each epoch to evaluate model performance\n",
        "#run_validation(model, val_dataloader, tokenizer_src, tokenizer_tgt, config['seq_len'], device, lambda msg: batch_iterator.write(msg), global_step, writer)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ifc8c6ptrHC0",
        "outputId": "7be12a87-acaa-4fc3-a291-9e404afc563c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using. device cuda\n",
            "Max length of Context sentence: 256\n",
            "Max length of Response sentence: 259\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Professing epoch 00: 100%|██████████| 227/227 [01:46<00:00,  2.13it/s, loss=6.750]\n",
            "Professing epoch 01: 100%|██████████| 227/227 [01:45<00:00,  2.15it/s, loss=6.408]\n",
            "Professing epoch 02: 100%|██████████| 227/227 [01:45<00:00,  2.15it/s, loss=5.677]\n",
            "Professing epoch 03: 100%|██████████| 227/227 [01:45<00:00,  2.15it/s, loss=5.650]\n",
            "Professing epoch 04: 100%|██████████| 227/227 [01:45<00:00,  2.15it/s, loss=5.416]\n",
            "Professing epoch 05: 100%|██████████| 227/227 [01:45<00:00,  2.15it/s, loss=5.699]\n",
            "Professing epoch 06: 100%|██████████| 227/227 [01:45<00:00,  2.15it/s, loss=5.393]\n",
            "Professing epoch 07: 100%|██████████| 227/227 [01:45<00:00,  2.15it/s, loss=4.901]\n",
            "Professing epoch 08: 100%|██████████| 227/227 [01:45<00:00,  2.15it/s, loss=5.259]\n",
            "Professing epoch 09: 100%|██████████| 227/227 [01:45<00:00,  2.15it/s, loss=5.151]\n",
            "Professing epoch 10: 100%|██████████| 227/227 [01:45<00:00,  2.16it/s, loss=4.790]\n",
            "Professing epoch 11: 100%|██████████| 227/227 [01:45<00:00,  2.15it/s, loss=4.429]\n",
            "Professing epoch 12: 100%|██████████| 227/227 [01:45<00:00,  2.16it/s, loss=4.170]\n",
            "Professing epoch 13: 100%|██████████| 227/227 [01:45<00:00,  2.15it/s, loss=4.200]\n",
            "Professing epoch 14: 100%|██████████| 227/227 [01:45<00:00,  2.15it/s, loss=3.785]\n",
            "Professing epoch 15: 100%|██████████| 227/227 [01:45<00:00,  2.15it/s, loss=4.221]\n",
            "Professing epoch 16: 100%|██████████| 227/227 [01:45<00:00,  2.15it/s, loss=3.549]\n",
            "Professing epoch 17: 100%|██████████| 227/227 [01:45<00:00,  2.16it/s, loss=3.658]\n",
            "Professing epoch 18:  14%|█▍        | 32/227 [00:15<01:30,  2.15it/s, loss=3.433]"
          ]
        }
      ],
      "source": [
        "import warnings\n",
        "\n",
        "if __name__=='__main__':\n",
        "    warnings.filterwarnings('ignore')\n",
        "    config=get_config() #retrieving config settings\n",
        "    train_model(config) # training model with config arguments"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# saving model\n",
        "model_filename=get_weights_file_path(config, f'{epoch:02d}')\n",
        "\n",
        "# writting current model state to the `model_filename`\n",
        "torch.save({\n",
        "            'epoch':epoch, # current epoch\n",
        "            'model_state_dict': model.state_dict(), # current model state\n",
        "            'optimizer_state_dict': optimizer.state_dict(), # current optimizer state\n",
        "            'global_step': global_step # current global step\n",
        "        }, model_filename)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "FM0wVzkGWV2p",
        "outputId": "85d252a4-2aa8-4bb2-d898-3f696e70e9f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'get_weights_file_path' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-fcf6ab92056f>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# saving model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel_filename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_weights_file_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'{epoch:02d}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# writting current model state to the `model_filename`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m torch.save({\n",
            "\u001b[0;31mNameError\u001b[0m: name 'get_weights_file_path' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "data = []\n",
        "for i in range(1000):\n",
        "  data.append(i)\n",
        "\n",
        "sample = DataLoader(data, batch_size=8, shuffle=True)"
      ],
      "metadata": {
        "id": "SfMRF7rNezEi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sample)"
      ],
      "metadata": {
        "id": "M5F7aoHNfQui",
        "outputId": "926d14e0-3008-4cd4-84a1-5261fca5dccd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<torch.utils.data.dataloader.DataLoader object at 0x7ed82f7d9a80>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4gAw2DXhrHC0"
      },
      "source": [
        "# Credit\n",
        "\n",
        "* https://www.kaggle.com/code/lusfernandotorres/transformer-from-scratch-with-pytorch/notebook?scriptVersionId=157547654\n",
        "* https://www.youtube.com/watch?v=ISNdQcPhsts&t=9595s\n",
        "* https://arxiv.org/pdf/1706.03762.pdf"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 30646,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c5bc0753284e4b74be15790051ef5740": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_18f1e1e2ae5d4e0287650abb8b4125db",
              "IPY_MODEL_6fa1028fd9594bcd9a9b9951d7c1203c",
              "IPY_MODEL_fb53c2b4f9e944ea9d8b94448d8bf912"
            ],
            "layout": "IPY_MODEL_cb51e462465e4cb0af34588aecde59b8"
          }
        },
        "18f1e1e2ae5d4e0287650abb8b4125db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_061e98be0cd5441aa802a9f75944b0ab",
            "placeholder": "​",
            "style": "IPY_MODEL_6ef47a1feae34ea1b48affab3ddef4c4",
            "value": "spiece.model: 100%"
          }
        },
        "6fa1028fd9594bcd9a9b9951d7c1203c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6e79fe118e924433b011dbc6f113bc6e",
            "max": 798011,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a77e0776db544ee0a55cfcc47e338832",
            "value": 798011
          }
        },
        "fb53c2b4f9e944ea9d8b94448d8bf912": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0c05d14ccb6b4055baafff9b8a831ae7",
            "placeholder": "​",
            "style": "IPY_MODEL_20556a94f21b4c9281980551aeb89e1f",
            "value": " 798k/798k [00:00&lt;00:00, 39.8MB/s]"
          }
        },
        "cb51e462465e4cb0af34588aecde59b8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "061e98be0cd5441aa802a9f75944b0ab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6ef47a1feae34ea1b48affab3ddef4c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6e79fe118e924433b011dbc6f113bc6e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a77e0776db544ee0a55cfcc47e338832": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0c05d14ccb6b4055baafff9b8a831ae7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "20556a94f21b4c9281980551aeb89e1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dd6d928046a14faf960dd1c27c795099": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_95b361b20fa044348b6504109378e417",
              "IPY_MODEL_319c2b9080dc4df48dc145b4018b9dd3",
              "IPY_MODEL_f542ba87319644d9860d81569626fd7a"
            ],
            "layout": "IPY_MODEL_6f9876bd1d21498095f87978b6da2880"
          }
        },
        "95b361b20fa044348b6504109378e417": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3ce0758821324f38af3a5d4ae881b2da",
            "placeholder": "​",
            "style": "IPY_MODEL_81798348ac8a46faaa2f53acecef2167",
            "value": "tokenizer.json: 100%"
          }
        },
        "319c2b9080dc4df48dc145b4018b9dd3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b32330d1558548e39e0dd7db8ad65b7b",
            "max": 1382015,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0a6d46f7ece84a7d80e5570124c23c70",
            "value": 1382015
          }
        },
        "f542ba87319644d9860d81569626fd7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7ca5ffbaaac6458c822acfc2780b3772",
            "placeholder": "​",
            "style": "IPY_MODEL_cbbef773527b48c5946dd104c445fc0b",
            "value": " 1.38M/1.38M [00:00&lt;00:00, 66.6MB/s]"
          }
        },
        "6f9876bd1d21498095f87978b6da2880": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3ce0758821324f38af3a5d4ae881b2da": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "81798348ac8a46faaa2f53acecef2167": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b32330d1558548e39e0dd7db8ad65b7b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0a6d46f7ece84a7d80e5570124c23c70": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7ca5ffbaaac6458c822acfc2780b3772": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cbbef773527b48c5946dd104c445fc0b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3462b7bd69484e75a37bb03262dd2f2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f6b514f0605a4751b0446aa4a41b2d09",
              "IPY_MODEL_43c80a5179934407b25b5e850a48ddc4",
              "IPY_MODEL_07b36552dbf44306b359fe2e4271c774"
            ],
            "layout": "IPY_MODEL_41b378532d2045ae839265b74fbbfa56"
          }
        },
        "f6b514f0605a4751b0446aa4a41b2d09": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8e2b9c432e6e4973b5502c4893283e34",
            "placeholder": "​",
            "style": "IPY_MODEL_74f802adf8b449d4a93af5dd49f3c4d5",
            "value": "config.json: 100%"
          }
        },
        "43c80a5179934407b25b5e850a48ddc4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b39408c90b9a4713bfe92846c75bf230",
            "max": 760,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9693a665ff63483785cdc5a9737cca01",
            "value": 760
          }
        },
        "07b36552dbf44306b359fe2e4271c774": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_66f32025acfc48b3b5e2736e6b0d6693",
            "placeholder": "​",
            "style": "IPY_MODEL_40f333864302417d96962b7de7b55524",
            "value": " 760/760 [00:00&lt;00:00, 43.5kB/s]"
          }
        },
        "41b378532d2045ae839265b74fbbfa56": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8e2b9c432e6e4973b5502c4893283e34": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "74f802adf8b449d4a93af5dd49f3c4d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b39408c90b9a4713bfe92846c75bf230": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9693a665ff63483785cdc5a9737cca01": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "66f32025acfc48b3b5e2736e6b0d6693": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "40f333864302417d96962b7de7b55524": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}